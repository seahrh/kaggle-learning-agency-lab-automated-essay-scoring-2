{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f8271d-6717-4d4f-ae7e-aafd88759d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import scml\n",
    "from scml import pandasx as pdx\n",
    "import lalaes2 as mylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a324e6-6fe8-4e00-96f4-f629f369e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"models/aes2/deberta_v3_base/20240628_111207\")\n",
    "validation_data_file = Path(\"input/val_06.parquet\")\n",
    "model_max_length = 768\n",
    "batch_size = 32\n",
    "critique_column = \"ctq_3_Qwen2-1.5B-Instruct\"\n",
    "#model_class = \"CustomDebertaV2ForTokenClassification\"\n",
    "model_class = \"auto\"\n",
    "if model_max_length==768:\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2aca212-9caa-4d92-ab0c-e18163541b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int16, min=-32768, max=32767\n"
     ]
    }
   ],
   "source": [
    "tim = scml.Timer()\n",
    "tim.start()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()\n",
    "info = np.iinfo(np.int16)\n",
    "print(f\"int16, min={info.min}, max={info.max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b4447a-4992-473d-aa75-13d53206d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=0, NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Mem Allocated: 0.0 GB\n",
      "Mem Cached:    0.0 GB\n",
      "device=1, NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Mem Allocated: 0.0 GB\n",
      "Mem Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    batch_size = 128\n",
    "    print(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"device={i}, {torch.cuda.get_device_name(i)}\")\n",
    "        print('Mem Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "        print('Mem Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')\n",
    "else:\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7c339b-79cd-447e-bcf4-cf67dbdc18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2094 entries, 0 to 2093\n",
      "Data columns (total 6 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   essay_id                   2094 non-null   object\n",
      " 1   score                      2094 non-null   int8  \n",
      " 2   ctq_3_Qwen2-1.5B-Instruct  2094 non-null   object\n",
      " 3   topic                      2094 non-null   object\n",
      " 4   full_text                  2094 non-null   object\n",
      " 5   source                     2094 non-null   object\n",
      "dtypes: int8(1), object(5)\n",
      "memory usage: 84.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(validation_data_file)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9072dac-39f6-433e-90c9-52838474b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2TokenizerFast(name_or_path='models/aes2/deberta_v3_base/20240628_111207', vocab_size=128000, model_max_length=768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pankun/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=model_max_length)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c30954-b834-48ce-a8f5-04aa2d6c4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ")\n",
      "CPU times: user 2.28 s, sys: 1.75 s, total: 4.04 s\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if model_class==\"CustomDebertaV2ForTokenClassification\":\n",
    "    #model = CustomDebertaV2ForTokenClassification.from_pretrained(model_dir)\n",
    "    raise ValueError\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "381307d0-e205-4eca-bff1-4cb614c1955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict hms score: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131/131 [00:47<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 338 ms, total: 50.1 s\n",
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = mylib.evaluation_aes2(\n",
    "    ds=mylib.Aes2Dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        critiques=df[critique_column].tolist(),\n",
    "        texts=df[\"full_text\"].tolist(),\n",
    "        labels=df[\"score\"].tolist(),\n",
    "    ),\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9658a174-593c-4595-9bd8-9a5ed456e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/aes2/deberta_v3_base/20240628_111207\n",
      "[1.498935947960958, 2.2397895160227517, 2.9740034895162504, 3.5223502418743555, 7.200203307066931]\n",
      "{\n",
      "  \"thresholds\": [\n",
      "    1.498935947960958,\n",
      "    2.2397895160227517,\n",
      "    2.9740034895162504,\n",
      "    3.5223502418743555,\n",
      "    7.200203307066931\n",
      "  ],\n",
      "  \"qwk\": 0.7574758007268526,\n",
      "  \"rmse\": 0.7466819718679459\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model_dir)\n",
    "print(res[\"thresholds\"])\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2b5bd2-3891-4e39-ad96-cdf65b507f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 0:01:47.697721\n"
     ]
    }
   ],
   "source": [
    "tim.stop()\n",
    "print(f\"Total time taken {str(tim.elapsed)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
