[DEFAULT]
home_dir=/mnt/c/github/seahrh/kaggle-learning-agency-lab-automated-essay-scoring-2
input_dir=${home_dir}/input/
output_dir=${home_dir}/output/
pretrained_dir=/mnt/c/huggingface/
tmp_dir=${home_dir}/tmp/
model_dir=${home_dir}/models/
teacher_dir=${home_dir}/teacher/
seed=31

[reduce_lr_on_plateau]
qualified_name=torch.optim.lr_scheduler.ReduceLROnPlateau
min_lr=1e-7
patience=3
factor=0.5
verbose=1

[cosine_annealing_lr]
qualified_name=torch.optim.lr_scheduler.CosineAnnealingLR
T_max=10
verbose=1

[swa_lr]
qualified_name=torch.optim.swa_utils.SWALR
swa_lr=5e-5
anneal_epochs=4
anneal_strategy=cos

[aes2]
#job_dir=${model_dir}aes2/deberta_v3_base/20240625_071352/
#resume_training_from=${job_dir}lightning_logs/version_0/checkpoints/epoch=2-step=2103-val_loss=1.02690.ckpt
gpus=1 0
train_strategy=ddp
# https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
train_precision=bf16-mixed
epochs=30
eval_every_n_steps=700
lr=5e-6
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1
batch_size=16
backbone=deberta_v3_base
model_max_length=512
stride=0
patience=4
gradient_checkpointing=0
hidden_dropout_prob=0
attention_probs_dropout_prob=0
critique_column=ctq_3_Qwen2-1.5B-Instruct
train_data_file=${input_dir}tra_04.parquet
train_data_first_n=0
validation_data_file=${input_dir}val_04.parquet
dataloader_num_workers=4
ckpt_filename=epoch={epoch}-step={step}-val_loss={val_loss:.5f}
ckpt_save_top_k=2
#model_class=CustomDebertaV2ForTokenClassification
model_class=auto
oof_enable=0
oof_epochs=2
oof_n_splits=2

[persuade]
#job_dir=${model_dir}aes2/deberta_v3_base/20240625_071352/
#resume_training_from=${job_dir}lightning_logs/version_0/checkpoints/epoch=2-step=2103-val_loss=1.02690.ckpt
gpus=1 0
train_strategy=ddp
# https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
train_precision=bf16-mixed
epochs=30
eval_every_n_steps=700
lr=1e-5
schedulers=reduce_lr_on_plateau
swa_start_epoch=-1
batch_size=16
backbone=deberta_v3_base
model_max_length=512
stride=0
patience=4
gradient_checkpointing=0
hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1
train_data_file=${input_dir}tra_05.parquet
train_data_first_n=0
validation_data_file=${input_dir}val_05.parquet
dataloader_num_workers=4
ckpt_filename=epoch={epoch}-step={step}-val_loss={val_loss:.5f}
ckpt_save_top_k=2
#model_class=CustomDebertaV2ForTokenClassification
model_class=auto

[deberta_v3_base]
directory=${pretrained_dir}microsoft/deberta-v3-base

[deberta_v3_large]
directory=${pretrained_dir}microsoft/deberta-v3-large

[roformer_base]
directory=${pretrained_dir}roformer-en-base

[longformer_base]
directory=${pretrained_dir}allenai/longformer-base-4096

[longformer_large]
directory=${pretrained_dir}allenai/longformer-large-4096

[llama2_7b]
directory=${pretrained_dir}meta-llama/Llama-2-7b-hf
